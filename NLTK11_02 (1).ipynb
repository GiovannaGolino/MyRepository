{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Complesso ventricolare in sede di ampiezza e morfologia regolare.\\r\\nRegolari le cisterne della basepi* ampi gli spazi liquorali corticali\\r\\ndegli spazi perivascolari trans parenchimali sopra e sottotentoriali.\\r\\n\\r\\nUn po' pi* accentuato che di norma il segnale sostanza bianca sia a\\r\\nlivello dei centri semiovali e della capsula interna del livello del\\r\\nponte.\\r\\n\\r\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file=codecs.open('C:/Users/Giova/Desktop/Referti_1011.txt.txt', encoding='iso-8859-1', mode='r')\n",
    "testo=file.read()\n",
    "testo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({' ': 46, 'e': 35, 'i': 35, 'a': 35, 'l': 30, 'o': 23, 'r': 18, 's': 17, 'n': 16, 't': 14, ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd=nltk.FreqDist(testo)\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Complesso ventricolare in sede di ampiezza e morfologia regolare.',\n",
       " 'Regolari le cisterne della basepi* ampi gli spazi liquorali corticali\\r\\ndegli spazi perivascolari trans parenchimali sopra e sottotentoriali.',\n",
       " \"Un po' pi* accentuato che di norma il segnale sostanza bianca sia a\\r\\nlivello dei centri semiovali e della capsula interna del livello del\\r\\nponte.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizzo il referto (divido il documento in frasi (con sent))\n",
    "frasi=sent_tokenize(testo) \n",
    "frasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Complesso',\n",
       " 'ventricolare',\n",
       " 'in',\n",
       " 'sede',\n",
       " 'di',\n",
       " 'ampiezza',\n",
       " 'e',\n",
       " 'morfologia',\n",
       " 'regolare',\n",
       " '.',\n",
       " 'Regolari',\n",
       " 'le',\n",
       " 'cisterne',\n",
       " 'della',\n",
       " 'basepi*',\n",
       " 'ampi',\n",
       " 'gli',\n",
       " 'spazi',\n",
       " 'liquorali',\n",
       " 'corticali',\n",
       " 'degli',\n",
       " 'spazi',\n",
       " 'perivascolari',\n",
       " 'trans',\n",
       " 'parenchimali',\n",
       " 'sopra',\n",
       " 'e',\n",
       " 'sottotentoriali',\n",
       " '.',\n",
       " 'Un',\n",
       " 'po',\n",
       " \"'\",\n",
       " 'pi*',\n",
       " 'accentuato',\n",
       " 'che',\n",
       " 'di',\n",
       " 'norma',\n",
       " 'il',\n",
       " 'segnale',\n",
       " 'sostanza',\n",
       " 'bianca',\n",
       " 'sia',\n",
       " 'a',\n",
       " 'livello',\n",
       " 'dei',\n",
       " 'centri',\n",
       " 'semiovali',\n",
       " 'e',\n",
       " 'della',\n",
       " 'capsula',\n",
       " 'interna',\n",
       " 'del',\n",
       " 'livello',\n",
       " 'del',\n",
       " 'ponte',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizzo il referto (divido il documento in parole e punteggiatura (con word))\n",
    "parole_e_punt= word_tokenize (testo)\n",
    "parole_e_punt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['complesso',\n",
       " 'ventricolare',\n",
       " 'in',\n",
       " 'sede',\n",
       " 'di',\n",
       " 'ampiezza',\n",
       " 'e',\n",
       " 'morfologia',\n",
       " 'regolare',\n",
       " 'regolari',\n",
       " 'le',\n",
       " 'cisterne',\n",
       " 'della',\n",
       " 'ampi',\n",
       " 'gli',\n",
       " 'spazi',\n",
       " 'liquorali',\n",
       " 'corticali',\n",
       " 'degli',\n",
       " 'spazi',\n",
       " 'perivascolari',\n",
       " 'trans',\n",
       " 'parenchimali',\n",
       " 'sopra',\n",
       " 'e',\n",
       " 'sottotentoriali',\n",
       " 'un',\n",
       " 'po',\n",
       " 'accentuato',\n",
       " 'che',\n",
       " 'di',\n",
       " 'norma',\n",
       " 'il',\n",
       " 'segnale',\n",
       " 'sostanza',\n",
       " 'bianca',\n",
       " 'sia',\n",
       " 'a',\n",
       " 'livello',\n",
       " 'dei',\n",
       " 'centri',\n",
       " 'semiovali',\n",
       " 'e',\n",
       " 'della',\n",
       " 'capsula',\n",
       " 'interna',\n",
       " 'del',\n",
       " 'livello',\n",
       " 'del',\n",
       " 'ponte']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#elimino la punteggiatura\n",
    "parole= [w.lower() for w in parole_e_punt if w.isalnum()]\n",
    "parole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'abbia',\n",
       " 'abbiamo',\n",
       " 'abbiano',\n",
       " 'abbiate',\n",
       " 'ad',\n",
       " 'agl',\n",
       " 'agli',\n",
       " 'ai',\n",
       " 'al',\n",
       " 'all',\n",
       " 'alla',\n",
       " 'alle',\n",
       " 'allo',\n",
       " 'anche',\n",
       " 'avemmo',\n",
       " 'avendo',\n",
       " 'avesse',\n",
       " 'avessero',\n",
       " 'avessi',\n",
       " 'avessimo',\n",
       " 'aveste',\n",
       " 'avesti',\n",
       " 'avete',\n",
       " 'aveva',\n",
       " 'avevamo',\n",
       " 'avevano',\n",
       " 'avevate',\n",
       " 'avevi',\n",
       " 'avevo',\n",
       " 'avrai',\n",
       " 'avranno',\n",
       " 'avrebbe',\n",
       " 'avrebbero',\n",
       " 'avrei',\n",
       " 'avremmo',\n",
       " 'avremo',\n",
       " 'avreste',\n",
       " 'avresti',\n",
       " 'avrete',\n",
       " 'avrà',\n",
       " 'avrò',\n",
       " 'avuta',\n",
       " 'avute',\n",
       " 'avuti',\n",
       " 'avuto',\n",
       " 'c',\n",
       " 'che',\n",
       " 'chi',\n",
       " 'ci',\n",
       " 'coi',\n",
       " 'col',\n",
       " 'come',\n",
       " 'con',\n",
       " 'contro',\n",
       " 'cui',\n",
       " 'da',\n",
       " 'dagl',\n",
       " 'dagli',\n",
       " 'dai',\n",
       " 'dal',\n",
       " 'dall',\n",
       " 'dalla',\n",
       " 'dalle',\n",
       " 'dallo',\n",
       " 'degl',\n",
       " 'degli',\n",
       " 'dei',\n",
       " 'del',\n",
       " 'dell',\n",
       " 'della',\n",
       " 'delle',\n",
       " 'dello',\n",
       " 'di',\n",
       " 'dov',\n",
       " 'dove',\n",
       " 'e',\n",
       " 'ebbe',\n",
       " 'ebbero',\n",
       " 'ebbi',\n",
       " 'ed',\n",
       " 'era',\n",
       " 'erano',\n",
       " 'eravamo',\n",
       " 'eravate',\n",
       " 'eri',\n",
       " 'ero',\n",
       " 'essendo',\n",
       " 'faccia',\n",
       " 'facciamo',\n",
       " 'facciano',\n",
       " 'facciate',\n",
       " 'faccio',\n",
       " 'facemmo',\n",
       " 'facendo',\n",
       " 'facesse',\n",
       " 'facessero',\n",
       " 'facessi',\n",
       " 'facessimo',\n",
       " 'faceste',\n",
       " 'facesti',\n",
       " 'faceva',\n",
       " 'facevamo',\n",
       " 'facevano',\n",
       " 'facevate',\n",
       " 'facevi',\n",
       " 'facevo',\n",
       " 'fai',\n",
       " 'fanno',\n",
       " 'farai',\n",
       " 'faranno',\n",
       " 'farebbe',\n",
       " 'farebbero',\n",
       " 'farei',\n",
       " 'faremmo',\n",
       " 'faremo',\n",
       " 'fareste',\n",
       " 'faresti',\n",
       " 'farete',\n",
       " 'farà',\n",
       " 'farò',\n",
       " 'fece',\n",
       " 'fecero',\n",
       " 'feci',\n",
       " 'fosse',\n",
       " 'fossero',\n",
       " 'fossi',\n",
       " 'fossimo',\n",
       " 'foste',\n",
       " 'fosti',\n",
       " 'fu',\n",
       " 'fui',\n",
       " 'fummo',\n",
       " 'furono',\n",
       " 'gli',\n",
       " 'ha',\n",
       " 'hai',\n",
       " 'hanno',\n",
       " 'ho',\n",
       " 'i',\n",
       " 'il',\n",
       " 'in',\n",
       " 'io',\n",
       " 'l',\n",
       " 'la',\n",
       " 'le',\n",
       " 'lei',\n",
       " 'li',\n",
       " 'lo',\n",
       " 'loro',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mi',\n",
       " 'mia',\n",
       " 'mie',\n",
       " 'miei',\n",
       " 'mio',\n",
       " 'ne',\n",
       " 'negl',\n",
       " 'negli',\n",
       " 'nei',\n",
       " 'nel',\n",
       " 'nell',\n",
       " 'nella',\n",
       " 'nelle',\n",
       " 'nello',\n",
       " 'noi',\n",
       " 'non',\n",
       " 'nostra',\n",
       " 'nostre',\n",
       " 'nostri',\n",
       " 'nostro',\n",
       " 'o',\n",
       " 'per',\n",
       " 'perché',\n",
       " 'più',\n",
       " 'quale',\n",
       " 'quanta',\n",
       " 'quante',\n",
       " 'quanti',\n",
       " 'quanto',\n",
       " 'quella',\n",
       " 'quelle',\n",
       " 'quelli',\n",
       " 'quello',\n",
       " 'questa',\n",
       " 'queste',\n",
       " 'questi',\n",
       " 'questo',\n",
       " 'sarai',\n",
       " 'saranno',\n",
       " 'sarebbe',\n",
       " 'sarebbero',\n",
       " 'sarei',\n",
       " 'saremmo',\n",
       " 'saremo',\n",
       " 'sareste',\n",
       " 'saresti',\n",
       " 'sarete',\n",
       " 'sarà',\n",
       " 'sarò',\n",
       " 'se',\n",
       " 'sei',\n",
       " 'si',\n",
       " 'sia',\n",
       " 'siamo',\n",
       " 'siano',\n",
       " 'siate',\n",
       " 'siete',\n",
       " 'sono',\n",
       " 'sta',\n",
       " 'stai',\n",
       " 'stando',\n",
       " 'stanno',\n",
       " 'starai',\n",
       " 'staranno',\n",
       " 'starebbe',\n",
       " 'starebbero',\n",
       " 'starei',\n",
       " 'staremmo',\n",
       " 'staremo',\n",
       " 'stareste',\n",
       " 'staresti',\n",
       " 'starete',\n",
       " 'starà',\n",
       " 'starò',\n",
       " 'stava',\n",
       " 'stavamo',\n",
       " 'stavano',\n",
       " 'stavate',\n",
       " 'stavi',\n",
       " 'stavo',\n",
       " 'stemmo',\n",
       " 'stesse',\n",
       " 'stessero',\n",
       " 'stessi',\n",
       " 'stessimo',\n",
       " 'steste',\n",
       " 'stesti',\n",
       " 'stette',\n",
       " 'stettero',\n",
       " 'stetti',\n",
       " 'stia',\n",
       " 'stiamo',\n",
       " 'stiano',\n",
       " 'stiate',\n",
       " 'sto',\n",
       " 'su',\n",
       " 'sua',\n",
       " 'sue',\n",
       " 'sugl',\n",
       " 'sugli',\n",
       " 'sui',\n",
       " 'sul',\n",
       " 'sull',\n",
       " 'sulla',\n",
       " 'sulle',\n",
       " 'sullo',\n",
       " 'suo',\n",
       " 'suoi',\n",
       " 'ti',\n",
       " 'tra',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tue',\n",
       " 'tuo',\n",
       " 'tuoi',\n",
       " 'tutti',\n",
       " 'tutto',\n",
       " 'un',\n",
       " 'una',\n",
       " 'uno',\n",
       " 'vi',\n",
       " 'voi',\n",
       " 'vostra',\n",
       " 'vostre',\n",
       " 'vostri',\n",
       " 'vostro',\n",
       " 'è'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stampo le stopWords\n",
    "stopWords= set(stopwords.words('italian'))\n",
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complesso', 'ventricolare', 'sede', 'ampiezza', 'morfologia', 'regolare', 'regolari', 'cisterne', 'ampi', 'spazi', 'liquorali', 'corticali', 'spazi', 'perivascolari', 'trans', 'parenchimali', 'sopra', 'sottotentoriali', 'po', 'accentuato', 'norma', 'segnale', 'sostanza', 'bianca', 'livello', 'centri', 'semiovali', 'capsula', 'interna', 'livello', 'ponte']\n"
     ]
    }
   ],
   "source": [
    "#elimino le stopWords\n",
    "\n",
    "wordsFiltered= []\n",
    "\n",
    "for w in parole:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "        \n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compless', 'ventricol', 'sed', 'ampiezz', 'morfolog', 'regol', 'regolar', 'cist', 'ampi', 'spaz', 'liquoral', 'cortical', 'spaz', 'perivascolar', 'trans', 'parenchimal', 'sopr', 'sottotentorial', 'po', 'accentu', 'norm', 'segnal', 'sostanz', 'bianc', 'livell', 'centr', 'semioval', 'capsul', 'intern', 'livell', 'pont']\n"
     ]
    }
   ],
   "source": [
    "#FASE DI STEMMING\n",
    "it= nltk.stem.snowball.ItalianStemmer()\n",
    "parole_stem= []\n",
    "for p in wordsFiltered:\n",
    "    f=it.stem(p)\n",
    "    parole_stem.append(f)\n",
    "print(parole_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compless', 'ventricol', 'sed', 'ampiezz', 'morfolog', 'regol', 'regolar', 'cist', 'ampi', 'spaz', 'liquoral', 'cortical', 'spaz', 'perivascolar', 'trans', 'parenchimal', 'sopr', 'sottotentorial', 'po', 'accentu', 'norm', 'segnal', 'sostanz', 'bianc', 'livell', 'centr', 'semioval', 'capsul', 'intern', 'livell', 'pont']\n"
     ]
    }
   ],
   "source": [
    "#FASE DI STEMMING in modo diverso\n",
    "italianStemmer=SnowballStemmer(\"italian\")\n",
    "parole_stem= []\n",
    "for p in wordsFiltered:\n",
    "    f=italianStemmer.stem(p)\n",
    "    parole_stem.append(f)\n",
    "print(parole_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compless', 'ventricol', 'in', 'sed', 'di', 'ampiezz', 'e', 'morfolog', 'regol', 'regolar', 'le', 'cist', 'della', 'ampi', 'gli', 'spaz', 'liquoral', 'cortical', 'degli', 'spaz', 'perivascolar', 'trans', 'parenchimal', 'sopr', 'e', 'sottotentorial', 'un', 'po', 'accentu', 'che', 'di', 'norm', 'il', 'segnal', 'sostanz', 'bianc', 'sia', 'a', 'livell', 'dei', 'centr', 'semioval', 'e', 'della', 'capsul', 'intern', 'del', 'livell', 'del', 'pont']\n"
     ]
    }
   ],
   "source": [
    "#Fase di stemming su parole con stop word\n",
    "italianStemmer=SnowballStemmer(\"italian\",ignore_stopwords=True)\n",
    "parole_stem_noSW= []\n",
    "for p in parole:\n",
    "    f=italianStemmer.stem(p)\n",
    "    parole_stem_noSW.append(f)\n",
    "print(parole_stem_noSW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compless', 'ventricol', 'sed', 'ampiezz', 'morfolog', 'regol', 'regolare', 'cist', 'ampio', 'spaz', 'liquoral', 'cortical', 'spaz', 'perivascolar', 'trans', 'parenchimal', 'sopr', 'sottotentorial', 'po', 'accentu', 'norm', 'segnal', 'sostanz', 'bianc', 'livell', 'centr', 'semioval', 'capsul', 'intern', 'livell', 'pont']\n"
     ]
    }
   ],
   "source": [
    "#LEMMATIZZAZIONE DELLE PAROLE DOPO LO STEMMING\n",
    "from pprint import pprint\n",
    "import treetaggerwrapper as ttpw\n",
    "tagger = ttpw.TreeTagger(TAGLANG='it', TAGDIR='C:\\TreeTagger')\n",
    "tags = tagger.tag_text(parole_stem)\n",
    "lemmas = [t.split('\\t')[-1] for t in tags]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['complesso', 'ventricolare', 'sede', 'ampiezza', 'morfologia', 'regolare', 'regolare', 'cisterna', 'ampio', 'spazio', 'liquorali', 'corticale', 'spazio', 'perivascolari', 'trans', 'parenchimali', 'sopra', 'sottotentoriali', 'po', 'accentuare', 'norma', 'segnale', 'sostanza', 'bianco', 'livello', 'centro', 'semiovali', 'capsula', 'interno', 'livello', 'ponte']\n"
     ]
    }
   ],
   "source": [
    "#LEMMATIZZAZIONE DELLE PAROLE PRIMA DELLO STEMMING\n",
    "from pprint import pprint\n",
    "import treetaggerwrapper as ttpw\n",
    "tagger = ttpw.TreeTagger(TAGLANG='it', TAGDIR='C:\\TreeTagger')\n",
    "#tags = tagger.tag_text(\"Ciao, questa frase è di prova\")\n",
    "tags = tagger.tag_text(wordsFiltered)\n",
    "lemmas = [t.split('\\t')[-1] for t in tags]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
